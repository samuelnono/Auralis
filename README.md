Roadmap
 Setup âœ…
Git + GitHub repository

Python virtual environment

Dependency management (requirements.txt)


Phase 1 â€” Core Audio Pipeline âœ…

Audio loading (WAV / MP3 supported)

MFCC feature extraction

Feature aggregation (mean + standard deviation)

Cosine similarity comparison


Phase 2 â€” Emotion Mapping (Interpretable) âœ…

Rule-based emotion inference from MFCC statistics

Multi-emotion scoring (calm, energetic, happy, sad)

Transparent score output


Phase 3 â€” Interactive Frontend âœ…

Streamlit web interface

Upload one or two audio files

Emotion prediction display

Similarity analysis

CSV experiment logging


Phase 4 â€” Ongoing Development â³

Expanded spectral features

Improved emotion heuristics

Scalable recommendation engine

Research evaluation and publication



ğŸ“Š Current System Capabilities

âœ” Audio feature extraction (MFCC-based)
âœ” Statistical feature aggregation
âœ” Interpretable emotion prediction
âœ” Cosine similarity comparison
âœ” Experiment logging (CSV export)
âœ” End-to-end Streamlit demo



ğŸ§  System Architecture

The system follows a modular, research-oriented pipeline.

User Upload (WAV / MP3)
        â”‚
        â–¼
Audio Loader (librosa)
        â”‚
        â–¼
MFCC Extraction
        â”‚
        â–¼
Feature Aggregation
(mean + std statistics)
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Cosine Similarity Module
        â”‚
        â–¼
Emotion Mapping Engine
(rule-based, interpretable)
        â”‚
        â–¼
Streamlit Frontend Output
- Emotion scores
- Predicted emotion
- Similarity score
- CSV logging


Each component is isolated and replaceable.
The feature extraction layer can later be swapped for deep embeddings.
The emotion mapping module can be replaced with a learned classifier.
The similarity engine can scale to large music databases.

The design is intentional: clarity first, complexity later.
